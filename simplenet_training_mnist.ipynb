{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Replicating the SimpleNet model architecture. \"\"\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import glorot_normal, RandomNormal, Zeros\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval & mean/std preprocess\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# create containers to solve negative dimensions of pooling due\n",
    "# to smaller images\n",
    "x_train_container = np.zeros((len(x_train), 32, 32, 1))\n",
    "x_test_container = np.zeros((len(x_test), 32, 32, 1))\n",
    "\n",
    "x_train = x_train.astype('float32').reshape([*x_train.shape, 1])\n",
    "x_test = x_test.astype('float32').reshape([*x_test.shape, 1])\n",
    "\n",
    "# put data into containers\n",
    "x_train_container[:, 2:30, 2:30, :] = x_train\n",
    "x_test_container[:, 2:30, 2:30, :] = x_test\n",
    "# rename containers to be the actual data\n",
    "x_train = x_train_container\n",
    "x_test  = x_test_container\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model architecture\n",
    "def create_model(s = 2, weight_decay = 1e-2, act=\"relu\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal(), input_shape=x_train.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 4\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    # First Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 5\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 6\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 7\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    # Second Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 8\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 9\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    \n",
    "    \n",
    "    # Block 10\n",
    "    model.add(Conv2D(512, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 11  \n",
    "    model.add(Conv2D(2048, (1,1), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 12  \n",
    "    model.add(Conv2D(256, (1,1), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fourth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    # Block 13\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fifth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "\n",
    "    # Final Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "468/468 [==============================] - 106s 227ms/step - loss: 12.0456 - acc: 0.6747 - val_loss: 9.9505 - val_acc: 0.4853\n",
      "Epoch 2/50\n",
      "468/468 [==============================] - 98s 209ms/step - loss: 6.4760 - acc: 0.9267 - val_loss: 5.2195 - val_acc: 0.7262\n",
      "Epoch 3/50\n",
      "468/468 [==============================] - 97s 207ms/step - loss: 3.4802 - acc: 0.9565 - val_loss: 2.5992 - val_acc: 0.9447\n",
      "Epoch 4/50\n",
      "468/468 [==============================] - 97s 207ms/step - loss: 1.9429 - acc: 0.9657 - val_loss: 1.4883 - val_acc: 0.9675\n",
      "Epoch 5/50\n",
      "468/468 [==============================] - 97s 208ms/step - loss: 1.1579 - acc: 0.9689 - val_loss: 0.8963 - val_acc: 0.9804\n",
      "Epoch 6/50\n",
      "333/468 [====================>.........] - ETA: 26s - loss: 0.7883 - acc: 0.9727"
     ]
    }
   ],
   "source": [
    "# Prepare for training \n",
    "model = create_model(act=\"relu\")\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "train = {}\n",
    "\n",
    "# First training for 50 epochs - (0-50)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_1\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=2*epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_1\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.5630 - acc: 0.9179 - val_loss: 0.5595 - val_acc: 0.92082:17 - los - ETA: 1:34 - loss: 0.603 - ETA: 1:06 - loss: 0.5934 - - ETA: 1:02 - loss - ETA: 58s - loss: 0.5873 - acc:  - ETA: 57s - loss: 0.58 - ETA: 11s - loss: - ETA: 8s - loss: 0.5649 - acc - ETA: 6s - lo - ETA: 1s - loss: 0.5630 - acc: 0. - ETA: 0s - loss: 0.5630 - acc: 0.\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.5339 - acc: 0.9257 - val_loss: 0.5404 - val_acc: 0.9253. - ETA: - ETA:\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.5138 - acc: 0.9314 - val_loss: 0.5480 - val_acc: 0.92178s - loss: 0.5136 - acc: - ETA: 7s - loss: 0.51 - ETA: 4s - los\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.5015 - acc: 0.9334 - val_loss: 0.5185 - val_acc: 0.93042s - loss: 0.501\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4928 - acc: 0.9339 - val_loss: 0.5178 - val_acc: 0.9283s -  - ETA: 2s - loss: 0.493\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4830 - acc: 0.9355 - val_loss: 0.5254 - val_acc: 0.9244\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4803 - acc: 0.9364 - val_loss: 0.5095 - val_acc: 0.9288\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4722 - acc: 0.9379 - val_loss: 0.5016 - val_acc: 0.9300\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4594 - acc: 0.9409 - val_loss: 0.4931 - val_acc: 0.93111 -  - ET - ETA - ETA: 41s - loss: 0.4597 - acc: 0.94 - ETA: 41s - loss: 0.45 - ETA: 24s - loss:  - ETA: 22s  - E - ETA: 16s - loss: 0. - ETA: 14s - loss: 0.4592 - - ETA: 13s - loss: 0.45 - ETA: \n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4566 - acc: 0.9408 - val_loss: 0.4875 - val_acc: 0.9331s: 0.4560 - acc: 0.94 - ETA: 1:07 - loss: 0.4570 - acc: - ETA: 1:06 - loss: 0.4565 - acc: 0.94 - ETA: 1:06 - loss: 0 - ETA: - ETA: 23s - loss: 0.4571 - ETA: 10s - lo - ETA: 5s - loss: 0.4562 - acc: 0.9 - ETA: 5s - loss: 0.4562 - acc: 0.941 - ETA: 4s - l\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4482 - acc: 0.9447 - val_loss: 0.4995 - val_acc: 0.9279 ETA: 43s  - ETA: 40s - loss: 0.4482 - a - ETA: 39s - loss: 0.4481 - acc - ETA: 35s - loss: 0.4491 - acc - - ETA: 4s - l\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4447 - acc: 0.9428 - val_loss: 0.4948 - val_acc: 0.9271ETA: 1:14 - loss: 0.4383 - acc: 0.944  - ETA: 35s - loss: 0. - ETA: 29s - loss: 0. - ETA - - ETA: 13s - lo - ET - ETA: 5s - loss: 0 - ETA: 1s - loss: 0.4438 - ac\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4393 - acc: 0.9431 - val_loss: 0.4807 - val_acc: 0.9328 14s - loss:  - ETA: 7s - loss: 0.4395 - acc - ETA: 5s -  - ETA: 0s - loss: 0.4391 - acc: 0.94 - ETA: 0s - loss: 0.4395 - acc: 0.943\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4344 - acc: 0.9445 - val_loss: 0.4868 - val_acc: 0.9303 ETA: 3s - loss: 0.4344 - acc: 0. - ETA: 2s - loss: 0.434\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4318 - acc: 0.9451 - val_loss: 0.4707 - val_acc: 0.9356A: 1:10 - loss: 0.4 - ETA: 1:07 - loss: 0.4269 - acc: 0. - ETA: 1:07 - loss: 0.4229 - acc - ETA: 1:05 - loss: 0.4268  - ETA: 1:03  - ETA: 54s - lo - ETA: 2s - loss: 0.4320 - acc - ETA: 0s - loss: 0.4322 - acc: 0.9\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4238 - acc: 0.9460 - val_loss: 0.4699 - val_acc: 0.9342s: 0.4235 - ETA: 14s - loss: 0.4244 - acc: 0.94 - ETA: 14s - loss:  - ETA: 12s - loss: 0.4241 - acc - ETA: 11s - loss: 0.4248 - acc - ETA: 10s - loss: 0.42 - ETA: 0s - loss: 0.4240 - acc: 0.\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.4187 - acc: 0.9484 - val_loss: 0.4819 - val_acc: 0.9287- loss: 0.4383 - acc:  - ETA: 1: - ETA: 54s -  - ETA: 52s - loss: 0.4156 - acc: 0.95 - ETA: 52s - loss: 0.4148 - - ETA: 51s - loss: 0.4147 - acc - ETA: 50s - loss: 0.4145 - a - ETA: 49 - ETA: 46s - loss: 0.4123 - - ETA: 45s - loss - ETA: 39s - loss: 0.41 - ETA: 33s - loss - ETA: 16s - loss: 0.4170 - acc - ETA: 15s - loss: 0.4169 - ETA: 14s - loss: 0.4171  - ETA: 7s - ETA: 2s - loss: 0.4189 - acc: 0.94 - ETA: 1s - loss: 0.4188 - ac\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4168 - acc: 0.9470 - val_loss: 0.4666 - val_acc: 0.9353: 0.4168 - acc: 0.94\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4117 - acc: 0.9476 - val_loss: 0.4686 - val_acc: 0.9326 11s - loss: 0.4118 - - ETA: 9s - loss: 0.4119 - acc - ETA: 0s - loss: 0.4116 - acc: 0.94 - ETA: 0s - loss: 0.4117 - acc: 0.947\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4051 - acc: 0.9488 - val_loss: 0.4537 - val_acc: 0.9363A: 10s - loss: 0.4051 - acc: 0. - ETA: 10s - loss: 0.4051\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3987 - acc: 0.9507 - val_loss: 0.4461 - val_acc: 0.9369oss: 0.3997 - - ETA: 14s - loss: 0. - ETA: 13s - loss: 0.3992 - ETA: 5s - loss: 0.3992 - acc - ETA: 4s - loss:\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3986 - acc: 0.9503 - val_loss: 0.4469 - val_acc: 0.9379\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3937 - acc: 0.9504 - val_loss: 0.4462 - val_acc: 0.9373: 5s - loss: - ETA: 1s - loss: 0.3936 - acc\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3911 - acc: 0.9502 - val_loss: 0.4467 - val_acc: 0.9367 lo - ETA: 12s -  - ETA: 10s - loss: 0.3907 - acc: - ETA: 9s - loss: 0.3906 - acc:  - ETA: 8s - loss: 0. - ETA: 4s - lo\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3873 - acc: 0.9517 - val_loss: 0.4549 - val_acc: 0.9325A: 47s -  - ETA: 36s -  - ETA: 34s - loss: 0.3866 - acc: 0. - ETA: 34s - lo - ETA:  - ETA - ETA: 21s - loss: 0.38 - ETA: 6s - loss: 0.3869 - acc: 0.95 - ETA: 6s -  - ETA: 0s - loss: 0.3874 - acc: 0\n",
      "{'val_loss': [0.5594911217212677, 0.540420709848404, 0.5480135992050171, 0.5184911138057708, 0.5177814515590667, 0.525421714925766, 0.5094652741909027, 0.501620380115509, 0.49307598848342893, 0.4874684368610382, 0.4995268879890442, 0.49480370492935183, 0.48067618737220763, 0.48681386733055115, 0.4706835277557373, 0.46991068778038025, 0.4818645356655121, 0.46655456433296205, 0.46862304348945616, 0.4537186270236969, 0.4460983902454376, 0.4468611700534821, 0.44618262605667114, 0.4467319087982178, 0.45487737202644346], 'val_acc': [0.9208, 0.9253, 0.9217, 0.9304, 0.9283, 0.9244, 0.9288, 0.93, 0.9311, 0.9331, 0.9279, 0.9271, 0.9328, 0.9303, 0.9356, 0.9342, 0.9287, 0.9353, 0.9326, 0.9363, 0.9369, 0.9379, 0.9373, 0.9367, 0.9325], 'loss': [0.5630146440787193, 0.533945812250048, 0.5137734133768127, 0.5015920430832962, 0.49283583376665396, 0.48303931829139213, 0.48028344080392316, 0.47222276592736523, 0.45929338941981024, 0.45648595737577213, 0.4482610299681332, 0.4445277555347598, 0.4393811016117949, 0.434287510604326, 0.4319000170614844, 0.4236878977588327, 0.4186646797009145, 0.41675711261334997, 0.41166652789244407, 0.4050657087826446, 0.3986831505137221, 0.398679058378539, 0.39366376061526714, 0.39110447703013634, 0.3872894434581631], 'acc': [0.9178685897435898, 0.9256697144881632, 0.9313843439586812, 0.9333293230288068, 0.9338506576836701, 0.9354748155086287, 0.9363570741097209, 0.9379210779595765, 0.9409889316268176, 0.9408485723452037, 0.9446382739431475, 0.9427935514535741, 0.9430542188383733, 0.9444578119987167, 0.9450794033106221, 0.9461020211550836, 0.9484079242670501, 0.947024382438115, 0.9476459736735308, 0.9488089509334632, 0.9506737247161999, 0.950252646737501, 0.9503529034136655, 0.9501924927432758, 0.9516762913248651]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more - (50-75)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_2\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_2\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3784 - acc: 0.9547 - val_loss: 0.4370 - val_acc: 0.9395 0.\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3737 - acc: 0.9560 - val_loss: 0.4357 - val_acc: 0.9402- ETA: 15s - loss: 0.3749 - ETA: 13s - loss: 0.37 - ETA: 12s -  - ETA: 2s - loss: 0.3739 - acc: - ETA: 0s - loss: 0.3737 - acc: 0.\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3727 - acc: 0.9564 - val_loss: 0.4369 - val_acc: 0.9399ETA: 1:10 - loss: 0.3772 - acc: 0.96 - ETA: 1:11 - los - ETA: 2s - loss: 0.3728 - acc: 0. - ETA: 1s - loss: 0.3729 - acc: 0. - ETA: 0s - loss: 0.3727 - acc: 0. - ETA: 0s - loss: 0.3726 - acc: 0.956\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3698 - acc: 0.9578 - val_loss: 0.4351 - val_acc: 0.93970s - loss: 0.3700 - acc: 0.95\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3693 - acc: 0.9577 - val_loss: 0.4341 - val_acc: 0.9398: 19s - loss: 0.3717 - ETA: 18s - loss - ETA - ETA: 12s - loss: 0.37 - ETA: 11s - loss: 0.3714  - ETA: 2s - loss: 0.3695 - a - ETA: 0s - loss: 0.3694 - acc: 0.95\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3700 - acc: 0.9572 - val_loss: 0.4313 - val_acc: 0.9405 46s - loss: 0.3749 - - ETA: 4s - loss:\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3690 - acc: 0.9572 - val_loss: 0.4342 - val_acc: 0.9408ETA: 1:11 - loss: 0.3 - ETA: 1:09 - loss: 0.3748 - acc: 0.959 - ETA: 1:09 - loss: 0.3749 - acc: 0 - ETA: 1:08 - loss: 0.3770  - ETA: 1:06 - loss: 0.3675 - a - ETA: 1:04 - loss: 0.3670 - acc: 0.95 - ETA: 1:04 - loss: 0.3656 - acc: 0 - ETA: 1:03 - loss: 0.365 - ETA: 52s - lo - ETA: 42 - ETA: 39s - loss - ETA: 33 - ETA - ETA: 19s - loss - ETA: 17s - loss: 0.3675 - acc:  - ETA: 7s - lo - ETA: 2s - loss: 0.368\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3667 - acc: 0.9578 - val_loss: 0.4348 - val_acc: 0.9400s: 0.3 - ETA: 1:05 - loss: 0.3721 - acc: 0.9 - ETA: 1:05 - loss: 0.3704 - acc: 0.9 - ETA: 1:04 - loss: 0.3705 - acc: 0.957 - ETA: 1:04 - loss: 0.3699 - acc: 0.957 - ETA: 1:04 - loss: 0.3695 - acc:  - ETA:  - ETA: 15s - loss: 0.3660 - ETA: 14s - loss: 0. - ETA: 12s  - ETA: 9s - loss: 0.3661 - acc: 0.95 - ETA: 9s - loss: 0.3661 - acc: 0.958 - ETA: 8s - loss: 0.3661 - acc - ETA: 7s - loss: 0.36 - ETA: 4s - loss\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3667 - acc: 0.9581 - val_loss: 0.4350 - val_acc: 0.9387\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3683 - acc: 0.9578 - val_loss: 0.4314 - val_acc: 0.9399 - loss: 0.3680 - acc: 0 - ETA: 0s - loss: 0.3684 - acc: 0.957\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3661 - acc: 0.9578 - val_loss: 0.4364 - val_acc: 0.9392loss: 0.3662 - acc: 0.9 - ETA: 0s - loss: 0.3662 - acc: 0.9\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3640 - acc: 0.9591 - val_loss: 0.4322 - val_acc: 0.9407 loss: 0.3608 - a - ETA: 27s  - ETA: 25s -  - ETA: 18 - ETA: 1\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3668 - acc: 0.9577 - val_loss: 0.4337 - val_acc: 0.9396s - loss: 0.3668 - acc - ETA: 1s - loss: 0.3666 - acc:\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3624 - acc: 0.9593 - val_loss: 0.4312 - val_acc: 0.9403- ETA: 20s - loss: 0.3620 - a - - ETA: 11s - loss - ETA: 9s - loss: 0.3624 - acc: 0.9 - ETA: - ETA: 2s - loss: 0.3623\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3676 - acc: 0.9569 - val_loss: 0.4330 - val_acc: 0.9393 - loss: 0.3685 - acc - ETA: 40s - loss: 0.3681 - acc:  - ETA: 40s - lo - ETA: 38s - loss: 0.3694 - a - ETA: 37s - loss - ETA: 34s - loss: 0.3696 - acc:  - ETA: 34s - loss: 0.3 - ETA: 1s - loss: 0.3678 - acc: 0 - ETA: 0s - loss: 0.3676 - acc: 0.956\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3633 - acc: 0.9589 - val_loss: 0.4315 - val_acc: 0.9396A:  - E - ETA: 9s - loss: 0.3637 - acc:  - ETA: 8s - loss: 0.363 - ETA: 5s - loss: 0.3628 -  - ETA: 3s - loss: 0.\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3596 - acc: 0.9598 - val_loss: 0.4320 - val_acc: 0.9388ss: 0.3609 -  - ETA: 48s - loss: 0.3608 - acc:  - ETA: 48s  - ETA: 18s - loss: 0.3599 - acc - ETA: 17s  - ETA: 11s - loss: 0.359 -  - ETA: 2s - loss: 0.3595 - acc: 0.959 - ETA: 2s - loss: 0.3594 - \n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3632 - acc: 0.9586 - val_loss: 0.4333 - val_acc: 0.9389\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3613 - acc: 0.9598 - val_loss: 0.4295 - val_acc: 0.9397  - ETA: 12s -  - ETA: 9s - loss: 0.3610 - acc: 0 - ETA: 8s - loss: 0.3 - ETA: 5s - loss: 0.3615 - acc:  - ETA: 4s - loss: 0.3617 - acc: 0.959 - ETA: 3s - loss: 0.3616  - ETA: 1s - loss: 0.3612 - acc:\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3587 - acc: 0.9598 - val_loss: 0.4312 - val_acc: 0.9400oss: 0.3586 - acc: 0.95\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3608 - acc: 0.9588 - val_loss: 0.4288 - val_acc: 0.9412\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3604 - acc: 0.9584 - val_loss: 0.4297 - val_acc: 0.9409c: 0.958 - ETA: 2s - loss: 0.3604 -\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3591 - acc: 0.9595 - val_loss: 0.4302 - val_acc: 0.94000. - ETA: 43s - loss: 0. - ETA: 42s - loss: 0.3605 - ETA: 40s - loss: 0.3598 - a - - ETA: 24s  - ETA: 13s - lo - ETA: 11s - loss: 0.3585 - acc: 0. - ETA: 11s - loss: 0.3585 - - ETA: 10s - loss: 0.3583 - acc: 0.9 - ETA: 9s - loss: 0.3583 - acc: 0. - ETA: 8s - loss:  - ETA: 5s - loss: 0.3583 - acc: 0. - ETA: 4s - loss: 0.3585 - - ETA: 1s - loss: 0.3587 - a\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3576 - acc: 0.9600 - val_loss: 0.4309 - val_acc: 0.9406. - ETA - ETA: 38s - loss: 0. - ETA: 24s - loss: 0.3577 - ETA:  - ETA:  - ETA - ETA: 9s - loss: 0.3581 - acc: 0 - ETA: 8s - loss: 0.3581 - acc: - - ETA: 0s - loss: 0.3576 - acc: 0.959\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3568 - acc: 0.9604 - val_loss: 0.4291 - val_acc: 0.9399\n",
      "{'val_loss': [0.43698849267959594, 0.43566109290122984, 0.43686207990646364, 0.4351421065330505, 0.4340761528015137, 0.4312846390247345, 0.4342050701141357, 0.4348142623901367, 0.43497609553337097, 0.4313897805213928, 0.4364479069709778, 0.43224830780029294, 0.433659273815155, 0.43121507840156553, 0.432983224773407, 0.43149812994003295, 0.43196640996932983, 0.43325454139709474, 0.4295385568618774, 0.43124645648002624, 0.42875798845291135, 0.42966645202636716, 0.43017170667648313, 0.4309269227027893, 0.4290649941444397], 'val_acc': [0.9395, 0.9402, 0.9399, 0.9397, 0.9398, 0.9405, 0.9408, 0.94, 0.9387, 0.9399, 0.9392, 0.9407, 0.9396, 0.9403, 0.9393, 0.9396, 0.9388, 0.9389, 0.9397, 0.94, 0.9412, 0.9409, 0.94, 0.9406, 0.9399], 'loss': [0.37841355089957895, 0.3736902769842628, 0.3726609191490665, 0.3697359597870353, 0.3693891435450455, 0.3700256641568149, 0.3688949816189479, 0.36668537385501193, 0.3667109216066519, 0.3683161935746612, 0.36604206624955066, 0.36401980971922887, 0.36675152628131885, 0.36228183411924664, 0.367634231439186, 0.3632343911210581, 0.3596778136818769, 0.3631735254278694, 0.3613147844115549, 0.3587239535814071, 0.3608053484080042, 0.3603481928392752, 0.359009022346704, 0.3575504442648816, 0.3567958417556819], 'acc': [0.9547275641025641, 0.955967276208019, 0.9564084055181264, 0.9578521013604092, 0.957671639396856, 0.9571904074048093, 0.957230510105871, 0.9577919473470615, 0.9580526146553707, 0.9578521013795316, 0.9578521013604092, 0.9590752326336892, 0.9577117420405503, 0.95931584861059, 0.9568695861022749, 0.9589348732755855, 0.9597770291947385, 0.9585538979405809, 0.9597970805643918, 0.9598171318958002, 0.9588346165802986, 0.9583934873084361, 0.9595364132178377, 0.9599775425088226, 0.9604186717806851]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more - (75-100)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_3\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_3\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.3579 - acc: 0.9593 - val_loss: 0.4289 - val_acc: 0.9407s: 0.35\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3547 - acc: 0.9611 - val_loss: 0.4295 - val_acc: 0.9403\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3573 - acc: 0.9614 - val_loss: 0.4288 - val_acc: 0.9401\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3567 - acc: 0.9605 - val_loss: 0.4289 - val_acc: 0.9404 5s - loss: 0.3569 -  - ETA: 3s - loss: 0.35\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3559 - acc: 0.9590 - val_loss: 0.4287 - val_acc: 0.94056 - acc: 0. - ETA: 1s - loss: 0.3556 - acc:\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3558 - acc: 0.9604 - val_loss: 0.4289 - val_acc: 0.9404: 1:03 - loss: 0.359 - ETA: 1:00  - ETA: 49s - loss: 0.35 - ETA: 48s - loss: 0.35 - ETA:  - ETA: 39s - loss:  - ETA:  - ETA: 30s - loss: 0.3540 - a - ET - ETA: 3s - loss: 0.3548 - acc: 0. - ETA: 2s - loss: 0.3553\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3581 - acc: 0.9597 - val_loss: 0.4283 - val_acc: 0.9409\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3544 - acc: 0.9616 - val_loss: 0.4283 - val_acc: 0.9406\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3598 - acc: 0.9591 - val_loss: 0.4290 - val_acc: 0.9405\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3576 - acc: 0.9595 - val_loss: 0.4287 - val_acc: 0.9400s - loss: 0.3560 - acc:  - ETA: 0s - loss: 0.3578 - acc: 0.\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3572 - acc: 0.9612 - val_loss: 0.4287 - val_acc: 0.940209 - - ETA: 1:06 - loss: 0.3628 - acc - ETA: 11s - loss: 0 - ETA: 9s - loss: 0.3574 - - ETA: 7s - loss: 0.3571 -  - ETA: 4s - l\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3555 - acc: 0.9607 - val_loss: 0.4294 - val_acc: 0.94003555 - - ETA: 30s - lo - ETA: 23s - lo - ETA: 17s - loss: 0.3544 - acc: 0. - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.3544 - acc:  - ETA: 15s - loss: 0. - ETA: 13s - loss: 0.3546 - acc:  - ETA:  - ETA: 1s - loss: 0.3550 - acc:\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3593 - acc: 0.9597 - val_loss: 0.4283 - val_acc: 0.9404.3593 - acc: 0. - ETA: 1s - loss: 0.3594 - ac\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3544 - acc: 0.9609 - val_loss: 0.4281 - val_acc: 0.9404TA: 1:11 - loss: 0.3569 - acc: 0.96 - ETA: 1:11 - loss: 0.3630 - ETA: 1:09 -  - ETA: 4s - los\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3562 - acc: 0.9610 - val_loss: 0.4282 - val_acc: 0.9408\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3560 - acc: 0.9605 - val_loss: 0.4290 - val_acc: 0.9402s - loss: 0 - ETA: 6s - loss: 0.3 - ETA: 3s - loss: 0.3\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3560 - acc: 0.9610 - val_loss: 0.4286 - val_acc: 0.9408.961 - ETA: 8s - loss: 0.3561 - acc: 0 - ETA: 7s - loss: 0.3560 - ETA: 4s - loss: 0.3560 - acc: 0.9 - ETA: 4s - loss:\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3561 - acc: 0.9609 - val_loss: 0.4293 - val_acc: 0.9399 0.3559 \n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3554 - acc: 0.9608 - val_loss: 0.4285 - val_acc: 0.9408s -\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3564 - acc: 0.9607 - val_loss: 0.4286 - val_acc: 0.9406 - ETA: 1s - loss: 0.3563 - acc:\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3544 - acc: 0.9606 - val_loss: 0.4288 - val_acc: 0.9406\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3560 - acc: 0.9607 - val_loss: 0.4280 - val_acc: 0.9404oss: 0.3563 - acc: 0.9\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3582 - acc: 0.9603 - val_loss: 0.4284 - val_acc: 0.9408\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3542 - acc: 0.9615 - val_loss: 0.4278 - val_acc: 0.9412\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3541 - acc: 0.9617 - val_loss: 0.4277 - val_acc: 0.9411A: 27s - loss: 0.3543 - acc:  - - ETA: 19s  - ETA: 12s - loss:  - ETA: 10s - loss: 0.3533 - acc: 0. - ETA: 10s - loss: 0.3 - ETA: 7s - loss: 0.3534 - acc:  - ET\n",
      "{'val_loss': [0.42890940618515017, 0.429476281785965, 0.4287737816810608, 0.42891113886833193, 0.4287343542575836, 0.4288902395248413, 0.42828984575271606, 0.4283210072517395, 0.42896775002479554, 0.4287059875488281, 0.42868418216705323, 0.42943099389076234, 0.42832710900306703, 0.42805296721458436, 0.42820959496498107, 0.42900065307617186, 0.4285962327480316, 0.42933241634368896, 0.428498144197464, 0.4285720456123352, 0.42881690883636475, 0.42801044244766234, 0.4283667218208313, 0.4278276875972748, 0.42774507489204405], 'val_acc': [0.9407, 0.9403, 0.9401, 0.9404, 0.9405, 0.9404, 0.9409, 0.9406, 0.9405, 0.94, 0.9402, 0.94, 0.9404, 0.9404, 0.9408, 0.9402, 0.9408, 0.9399, 0.9408, 0.9406, 0.9406, 0.9404, 0.9408, 0.9412, 0.9411], 'loss': [0.35788610722774117, 0.35471735087235, 0.35739470235728515, 0.35658860976282203, 0.3559105258397961, 0.3557594267554034, 0.35813543530354947, 0.35444176422696794, 0.3596729396205703, 0.3576442110048948, 0.35712298271860266, 0.35555150925176127, 0.35929490080390714, 0.3544595976708828, 0.3561909338354797, 0.3559790760288263, 0.3559313318382902, 0.35606786016856473, 0.3554000453207789, 0.3564172502622644, 0.35445933168730676, 0.356003798083078, 0.35817189386038584, 0.3542086050837018, 0.35403610186046003], 'acc': [0.9592948717948718, 0.9611405197113877, 0.9613811357265334, 0.9604988771254411, 0.9590150786012192, 0.9603986204492767, 0.9596767725759414, 0.9616217517225567, 0.9591754892907313, 0.9595163618673068, 0.9612006737438578, 0.9606392364452999, 0.9596767725376965, 0.9609199551232626, 0.9610202117038148, 0.9605590311005439, 0.9610402630352232, 0.9608999037536092, 0.9608196984662205, 0.9606592878149532, 0.9606392364452999, 0.9606392364835449, 0.9602783124608262, 0.9614813924026979, 0.9617220083413539]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more  - (100-125)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1*0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_4\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_4\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Final Logs:  {'part_1': <keras.callbacks.History object at 0x000002DBAE21FE80>, 'part_2': <keras.callbacks.History object at 0x000002DBB1226940>, 'part_3': <keras.callbacks.History object at 0x000002DBAE242D68>, 'part_4': <keras.callbacks.History object at 0x000002DBF553F240>}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n \\n Final Logs: \", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
