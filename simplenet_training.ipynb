{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Replicating the SimpleNet model architecture. \"\"\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import glorot_normal, RandomNormal, Zeros\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval & mean/std preprocess\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model architecture\n",
    "def create_model(s = 2, weight_decay = 1e-2, act=\"relu\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal(), input_shape=x_train.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 4\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    # First Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 5\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 6\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 7\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    # Second Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 8\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 9\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    \n",
    "    \n",
    "    # Block 10\n",
    "    model.add(Conv2D(512, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 11  \n",
    "    model.add(Conv2D(2048, (1,1), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 12  \n",
    "    model.add(Conv2D(256, (1,1), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fourth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    # Block 13\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fifth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "\n",
    "    # Final Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "390/390 [==============================] - 87s 222ms/step - loss: 13.3953 - acc: 0.3293 - val_loss: 12.7298 - val_acc: 0.1370  - ETA: 35s - loss: 14.5209 -  - ETA: 33s - loss - E - ETA: 1s - loss: 13.4416\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - 81s 208ms/step - loss: 8.8189 - acc: 0.4945 - val_loss: 8.5696 - val_acc: 0.183313 - acc:  - ETA: 5s - loss - ETA: 0s - loss: 8.8369 - acc: 0.494 - ETA: 0s - loss: 8.8326 - acc: 0.4\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 5.8523 - acc: 0.5925 - val_loss: 5.2709 - val_acc: 0.4896loss: 7.0415 - acc:  - ETA: 1:10 - loss: 7.0098 - acc: 0.548 - ETA: 1:10 - loss: 7.0072 -  - ETA: 1:08 - loss: 6.9498 - acc: - ETA - ETA: 56s  - ETA: 53s - loss: 6.6862 - acc: 0. - ETA: 53s - loss: 6.6793 - acc: 0. - ETA: 53s -  - ETA:  - ETA: 43s - loss: 6.5115 - acc:  - ETA: 42s - loss: 6.50 - ETA: 37s - loss: 6.4072 \n",
      "Epoch 4/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 4.0654 - acc: 0.6564 - val_loss: 3.7458 - val_acc: 0.5930\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 2.9949 - acc: 0.7028 - val_loss: 2.7542 - val_acc: 0.6704\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 2.3412 - acc: 0.7341 - val_loss: 2.1668 - val_acc: 0.7256\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.9183 - acc: 0.7564 - val_loss: 1.8689 - val_acc: 0.7381\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.6379 - acc: 0.7714 - val_loss: 1.6210 - val_acc: 0.7487\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.4383 - acc: 0.7844 - val_loss: 1.7578 - val_acc: 0.6655- a - ETA: 20s - loss: 1.4566 - a - ETA: 19s - lo - E - ETA: 14s  - ETA: 11s - loss: 1.4471 - ETA: 9s - loss: - ETA: 5s - loss: 1. - ETA: 2s - loss: 1.4397 - \n",
      "Epoch 10/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.3012 - acc: 0.7929 - val_loss: 1.6428 - val_acc: 0.6934A: 25s - loss: 1.3208 - - ETA: 24s - loss: 1.3194 - acc: 0. - - ETA: 16s - loss: 1.3151 - ETA: 15s - loss: 1.3138 - a - ETA: 14s - loss: 1.3132 - acc - - ETA: 9s - loss: 1.310 - ETA: 6s - - ETA: 1s - loss: 1.3025 - acc: 0.792 - ETA: 1s - loss: 1.3025 - acc: 0. - ETA: 0s - loss: 1.3017 - acc: 0.79 - ETA: 0s - loss: 1.3015 - acc: 0.792\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.1913 - acc: 0.8044 - val_loss: 1.2342 - val_acc: 0.7820c: 0. - ETA:  - ETA: 17s - loss: 1.1970 - acc: 0.80 - ETA: 17s - loss: 1.19 - ETA - ETA: 12s - loss: 1.1980 - ETA: 10s - loss: 1.196 - ETA: 8s - loss: 1.1950 - acc: 0 - ETA: 0s - loss: 1.1913 - acc: 0.80\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.1150 - acc: 0.8106 - val_loss: 1.0402 - val_acc: 0.8323 loss: 1.1231 - acc - ETA: 15 - ETA: 12s - loss: 1.1222 - a - ETA: 11s - l - E - ETA: 1s - loss: 1.1158 - acc:\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.0456 - acc: 0.8197 - val_loss: 1.0917 - val_acc: 0.7971 1.0757 - acc - ETA: 1:06 - loss: 1.0783 - - ETA: 1:04 - loss: 1.0707 - acc: 0.81 - ETA: 1:03 - loss: 1.06 - ETA: 56s - loss: 1.0657 - a - ETA - ETA - ETA: 33s - loss:  - ETA: 31s\n",
      "Epoch 14/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.9981 - acc: 0.8225 - val_loss: 1.0844 - val_acc: 0.7833 loss: 1.0054 - acc: 0. - ETA: 20s - loss: 1. - ETA: 19s - loss: 1.0060 - a - ETA:  - ETA: 15s -  - ETA: 12s - loss: 1. - ETA: 10s - loss: 1.0016 - acc:  - ETA: 10s - loss: 1.002 - ETA: 7s - loss: 1.0017 - ETA: 4s - lo\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.9583 - acc: 0.8286 - val_loss: 1.1253 - val_acc: 0.7798- loss: 0. - ETA: 44s - loss - ETA: 41s - loss: 0. - ETA - ETA: 33s - loss: 0.9673 - acc:  - ETA: 32s - loss: 0.9674 - - ETA: 31s - loss - ETA: 29s - loss: 0.9691 - - ETA: 27s - loss: 0. - ETA: 18s - loss:  - ETA: 16s - loss: 0. - ETA: 14s - loss: 0.9620 - - ETA: 13s - loss: 0.9618 - ETA: - ETA - ETA: 1s - loss: 0.9594 - acc:  - ETA: 0s - loss: 0.9584 - acc: 0.828\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.9244 - acc: 0.8324 - val_loss: 1.0071 - val_acc: 0.8038A: 1:10 - loss: 0.9464 - acc: - ETA: 57s - loss: 0.938\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.9008 - acc: 0.8352 - val_loss: 0.9830 - val_acc: 0.8027A: 1:06 - - ETA: 1:01 - l - ETA: 50s - loss - ETA: 48s - loss: 0.9036 - acc: 0.83 - ETA: 48s - loss:  - ETA: 42s - loss: 0.90 - ETA: 40s - loss: 0.9063 - a - ETA: 39s - loss:  - ETA: 37s - loss: 0.9047 - - ETA: 36s - loss: 0. - ETA: 34s - loss: 0.9019 - a - ETA: 34s - loss: 0.9026 - acc: 0.83 - ETA: 33 - ETA: 15s - loss:  - ETA: 13s - loss: 0.89 - ETA:  - ETA: 7s - loss: 0.8983 - acc: 0.836 - ETA: 7s - loss: 0.8983 - acc:  - ETA: 5s - loss: 0.8985  - ETA: 3s - loss: 0.9004 - acc: 0.835 - ETA: 3s - loss: 0.9006 - acc: 0.8 - ETA: 2s - loss: 0.9007 - acc: 0. - ETA: 1s - loss: 0.8998 - acc: 0.83 - ETA: 1s - loss: 0.9000 - acc:\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8715 - acc: 0.8394 - val_loss: 1.0031 - val_acc: 0.79820.873 - ETA: 1:07 - loss: 0.8794 - acc: 0. - ETA: 1:06 - - ETA: 1:01 - loss: 0.8758 - acc: 0. - ETA: 1:01 - lo - ETA: 30s -  - ETA: 28s - loss: 0.8714 - acc - ETA: 27s\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8519 - acc: 0.8420 - val_loss: 0.8147 - val_acc: 0.8556c - ETA: 18s - loss: 0.8507 - ETA: 17s - loss: 0. - ETA:  - ETA: 12s - loss: 0.85 - ETA: 3s - loss: 0.8\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8282 - acc: 0.8468 - val_loss: 0.8312 - val_acc: 0.8465 acc: 0 - ET\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8165 - acc: 0.8490 - val_loss: 0.9606 - val_acc: 0.803710 - loss: 0.8440 - acc: 0.8 - ETA: 1:09 - loss: 0.8403 - acc:  - ETA: 1:08 - loss: 0.8335 - acc: 0.848 - ETA: 1:08 - loss: 0.8343 - ETA: 1:06 - loss: 0.8183 - - ETA: 1:04 - loss: 0.8119 - acc:  - ETA: 1:02 - l - ETA: 55s - loss: 0.8162 - acc - ETA: 54s - loss: 0.8173 - acc: 0.85 - ETA: 54s - loss: 0. - ETA: 52s - loss: 0. - ETA: 50s - loss: 0.8156 - acc - ETA: 49s - loss:  - ETA: 7s - l - ETA: 2s - loss: 0.8165 - acc - ETA: 1s - loss: 0.8168 - acc: \n",
      "Epoch 22/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8020 - acc: 0.8518 - val_loss: 0.9569 - val_acc: 0.8077- acc: 0 - ETA: 4s - l\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7859 - acc: 0.8543 - val_loss: 0.8472 - val_acc: 0.83952 - loss: 0.8097 - acc: 0 - ETA: 1:10 - los - ETA: 1:06 - loss: 0.7819 - - E - ETA: 15s - l - ETA: 8s - loss:  - ETA: 4s - loss: 0.7854 - acc: 0.8 - ETA: 4s - loss: 0.7853 - acc:  - ETA: 2s - loss: 0.786\n",
      "Epoch 24/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7744 - acc: 0.8556 - val_loss: 0.8314 - val_acc: 0.8413ETA: 1:16 - loss: 0.7487 - acc: 0.8 - ETA: 1:15 - loss: 0.7581 - acc: 0.86 - ETA: 1:14 - loss: 0.8152 - acc: - ETA: 1:13  - ETA: 55s - lo - ETA: 41s - loss:  - - ETA: 36s  - ETA: 25s - loss: 0.7743 - acc:  - ETA: 25s - loss: 0. - ETA - ETA: 19s  - ETA: 17s - loss: 0.7733 - a - ETA: 6s - loss: 0.77 - ETA: 3s - loss: 0\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7684 - acc: 0.8570 - val_loss: 1.0783 - val_acc: 0.7549ETA: 1:14 - loss: 0.7573 - ETA: 1:12 - loss: 0.793 - ETA: 1:08 - loss: 0.7 - ETA: 1:05 - loss: - ETA: 1:01 - loss: 0.7691 - acc: 0. - ETA: 1:00 - loss: 0. - ETA: 58s - loss: 0.7694 - - ETA: 57s - loss: 0.7701 - ETA: 48s - loss: 0.7 - ETA: 5s - loss: 0.7668 - ac - ETA: 3s - loss: 0.7680 - ETA: 0s - loss: 0.7685 - acc: 0.8\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7561 - acc: 0.8608 - val_loss: 0.7841 - val_acc: 0.8532ETA: 10s -  - ETA: 6s - los - ETA: 1s - loss: 0.7557 - ac - ETA: 0s - loss: 0.7561 - acc: 0.860\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7491 - acc: 0.8602 - val_loss: 0.8493 - val_acc: 0.8336\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7421 - acc: 0.8628 - val_loss: 0.7858 - val_acc: 0.847347s - loss: 0.7372 - acc: 0.86 - ETA: 46s  - ETA: 44s - loss: 0.7377 - a - ETA: 43s  - ETA - ETA: 21s - loss: 0. - ETA: 16s - loss: 0.74 - ETA: 14s  - ETA: 11s - loss:  - ETA: 9s - loss: 0.7400 -  - ETA: 7s - loss: 0.7408 - - ETA: 5s - loss - ETA: 0s - loss: 0.7421 - acc: 0.\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7372 - acc: 0.8632 - val_loss: 0.8400 - val_acc: 0.827016s - loss: 0.7375 - acc - - ETA: 5s - loss: 0. - ETA: 1s - loss: 0.7381 - acc: 0.86 - ETA: 1s - loss: 0.7377 - acc:\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7348 - acc: 0.8631 - val_loss: 0.8577 - val_acc: 0.82303 - ETA: 17s - loss: 0. - ETA: 16s - loss: 0.7319 - a - ETA: 15s -  -  - ETA: 0s - loss: 0.7346 - acc: 0.8\n",
      "Epoch 31/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7293 - acc: 0.8647 - val_loss: 0.7821 - val_acc: 0.8512\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7178 - acc: 0.8700 - val_loss: 0.7114 - val_acc: 0.8715 - loss: 0.7177 - acc: 0.86 - ETA: 7s - l - ETA: 2s - loss: 0.7168 - acc: 0.87 - ETA: 2s - loss: 0.7167 \n",
      "Epoch 33/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7176 - acc: 0.8672 - val_loss: 1.0550 - val_acc: 0.76795s - loss: 0.7168 - acc: 0.867 - ETA: 5s -\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7113 - acc: 0.8712 - val_loss: 0.7650 - val_acc: 0.8468621  - ETA: 1: - ETA: 17s - loss: 0. - ETA: 6s - loss - ETA: 2s - loss: 0.7116 - \n",
      "Epoch 35/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7101 - acc: 0.8704 - val_loss: 0.7430 - val_acc: 0.8631 acc:  - ETA: 8s - loss: 0.7086 - acc: 0.8 - ETA: 7s - loss: 0.7090 - acc: 0.870 - ETA: 7s - loss:  - ETA: 3s - loss: 0.7091 - ac - ETA: 1s - loss: 0.7094 - acc\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7022 - acc: 0.8734 - val_loss: 0.7612 - val_acc: 0.8531TA: 1 - ETA: 1:07 - los - ETA: 1:03 - loss: 0.7175 - acc: 0 - ETA: 1:02 -  - ETA: 58 - ETA - ETA: 37s - loss:  - ETA: 35s - loss: 0. - ETA: 33s - loss: 0.7013 - - E - ETA: 24s - loss: 0.7007 - acc - ETA: 24s - loss: 0.7017 - ETA: 22s - loss: 0.7020 - acc: 0. - - ETA: 10s - loss: 0.6996 - acc - ETA: 9s - loss: 0.6996 - acc: 0.8 - ETA: 9s - loss: 0.7001 - acc: 0.873 - ETA: 8s - loss: 0.6998 - acc: 0.87 - ETA: 8s - loss: 0.7000 - acc: - ETA: 7s - l - ETA: 2s - loss: 0.7016 - acc: 0 - ETA: 1s - loss: 0.7019 - acc:\n",
      "Epoch 37/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.7008 - acc: 0.8731 - val_loss: 0.8182 - val_acc: 0.8326 13s - loss: 0.7037 - acc:  - ETA: - ETA: 0s - loss: 0.7005 - acc: 0\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6965 - acc: 0.8738 - val_loss: 1.0436 - val_acc: 0.7605ETA: 1:15 - l - ETA: 13s - loss: 0.6963 - acc: 0. - ETA - ETA: 2s - loss: 0.6959 - \n",
      "Epoch 39/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6908 - acc: 0.8766 - val_loss: 0.8446 - val_acc: 0.8265ETA: 1:14 - loss: 0.66 - ETA: 1:11 - loss: 0.6827 - a - ETA: 1:09 - loss: 0.6852 - acc: 0.87 - ETA: 1: - ETA: 1:02 - loss: 0.68 - ETA: 59s - loss: 0.6842 - acc - ETA: 51s - loss: 0.6871 - acc - ETA: 46s - loss: 0.6847 - acc: 0.87 - ETA - E - ETA: 6s - loss: 0.6922 -  - ETA: 4s - loss: 0.6912 - - ETA: 1s - loss: 0.6910 - acc:  - ETA: 0s - loss: 0.6904 - acc: 0.\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6935 - acc: 0.8738 - val_loss: 0.7951 - val_acc: 0.8484: 1:13 - loss: 0.6878 - acc: 0.87 - ETA: 1:13 - loss: 0.6878 - acc: 0.8 - ETA: 46s  - ETA: 35s - loss:  - ETA: 33s - loss: 0.6893 - acc - ETA: 33s - loss: 0.6896 - acc: 0. - - ETA: 29s - loss: 0.6891 - - ETA: 20s -  - ETA: 1 - ETA: 3s - loss: 0.6931 - acc: 0.8 - ETA: 2s - loss: 0.6934 - ac - ETA: 1s - loss: 0.6932 - acc: \n",
      "Epoch 41/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6857 - acc: 0.8772 - val_loss: 0.7622 - val_acc: 0.8491 0.6845 - - ETA: 12s - loss: 0.6846 - acc: 0.87 - ETA: 12s -  - ETA: 2s - loss: 0.6874 \n",
      "Epoch 42/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6870 - acc: 0.8771 - val_loss: 0.8379 - val_acc: 0.8333: 1:13 - ETA: 1:07 - loss: 0.7060 - acc: 0. - ETA:  - ETA: 24s  - ETA: 18s - loss: 0.6868 - acc - ETA: 17s - loss:  - ETA: 15s - loss - ETA: 13s - loss: 0.6891 - acc - ETA: 12s - loss: 0.6888 - acc - ETA: 11s - loss: 0.6881 - ETA: 10s - loss: 0.6 - ETA: 7s - loss: 0.6880 - acc:  - ETA: 6s - loss: 0.6891 - - ETA: 4s - loss: 0.6875 - - ETA: 1s - loss: 0.6865 - a\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6792 - acc: 0.8812 - val_loss: 0.7027 - val_acc: 0.8746 loss: 0.6911 - ac - - ETA: 16s - loss: 0.6738  - ETA: 5s - loss: 0.6777 - acc: 0. - ETA: 4s - loss\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6790 - acc: 0.8787 - val_loss: 0.6946 - val_acc: 0.8765\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6744 - acc: 0.8814 - val_loss: 0.7257 - val_acc: 0.8684 0.8 - ETA: 8s - loss: 0.6731 - ETA: 6s -  - ETA: 0s - loss: 0.6738 - acc: 0\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6774 - acc: 0.8806 - val_loss: 0.7612 - val_acc: 0.8525A: 1:13 - loss: 0.6913 - ac - ETA: 30s - loss: 0.6776 - acc:  - ETA: 29 - ETA: 27s -  - ETA: 24s - loss: 0.6778 - - ETA: 15s\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.6711 - acc: 0.8819 - val_loss: 0.6727 - val_acc: 0.8838- loss: 0.6708 - acc:  - ETA: 3s - loss: 0.6705 - ETA: 0s - loss: 0.6711 - acc: 0\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.6708 - acc: 0.8822 - val_loss: 0.8736 - val_acc: 0.8174 0.6715 - \n",
      "Epoch 49/50\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.6655 - acc: 0.8833 - val_loss: 0.7229 - val_acc: 0.8699 54s - lo - ETA:  - ETA: 22s - loss: 0.6\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.6692 - acc: 0.8823 - val_loss: 0.7965 - val_acc: 0.8422TA: 1:14 - loss: 0.6913 - acc: 0.8 - ETA: 1:13 - loss: 0.6796 - ETA: 1:10 - loss: 0.6788 - acc:  - ETA: 1:09 - loss: 0.6885 - acc - ETA: 1:08 - loss: 0.6830 - acc: 0.87 - ETA: 1 - ETA: 1:01 - loss: 0.6673 - acc: 0.882 - ETA: 1:01 - loss: 0.6676 - acc: 0. - ETA: 1:00 - loss: 0.6733 - ac\n",
      "{'val_loss': [12.729789875793458, 8.569569955444337, 5.270930777740478, 3.7458066287994383, 2.754214940261841, 2.166785647201538, 1.86885320186615, 1.6210223821640015, 1.7577783411026, 1.642753636932373, 1.234185137939453, 1.040206980419159, 1.0916515867233276, 1.0844478372573854, 1.1253063070297242, 1.0071297359466553, 0.9830351371765137, 1.003134217643738, 0.8146508083343506, 0.8312474913597107, 0.960564713382721, 0.9569149703025818, 0.8471581637382507, 0.8313701725959778, 1.078312646961212, 0.7841094984054565, 0.8492847093582153, 0.7858104717254639, 0.8399948547363282, 0.8576622997283936, 0.7821344769477844, 0.7114395176887512, 1.0550370407104492, 0.7649618613243103, 0.7430281121253968, 0.7612312244415284, 0.8182226688385009, 1.0435743111610412, 0.8446158561706543, 0.795140273475647, 0.7622321113586426, 0.8379416533470154, 0.7027402432441712, 0.6945795921325684, 0.72573374710083, 0.7612014866828919, 0.6727343234062195, 0.8735563439369202, 0.722864788722992, 0.7965400762557984], 'val_acc': [0.137, 0.1833, 0.4896, 0.593, 0.6704, 0.7256, 0.7381, 0.7487, 0.6655, 0.6934, 0.782, 0.8323, 0.7971, 0.7833, 0.7798, 0.8038, 0.8027, 0.7982, 0.8556, 0.8465, 0.8037, 0.8077, 0.8395, 0.8413, 0.7549, 0.8532, 0.8336, 0.8473, 0.827, 0.823, 0.8512, 0.8715, 0.7679, 0.8468, 0.8631, 0.8531, 0.8326, 0.7605, 0.8265, 0.8484, 0.8491, 0.8333, 0.8746, 0.8765, 0.8684, 0.8525, 0.8838, 0.8174, 0.8699, 0.8422], 'loss': [13.395313174907978, 8.81689994825474, 5.851021431025535, 4.064696724383465, 2.9943817077939006, 2.340882316705008, 1.9183049397779115, 1.6378535010472757, 1.4383447364541557, 1.3011273885714079, 1.1912467775721178, 1.1149122468699313, 1.0456723875252911, 0.9980841880813026, 0.9582868714649249, 0.9243747865996913, 0.9009113732843702, 0.8714873451317661, 0.8516828575155695, 0.8280468199893302, 0.8165792125442451, 0.8020224804139344, 0.7859568938369127, 0.7743697810677868, 0.7684020308068367, 0.756084153810525, 0.7491276996388893, 0.7422113630455734, 0.7371053886711885, 0.7347990448825972, 0.7293416196274229, 0.7178556507965446, 0.7175910467898039, 0.7112012015423179, 0.7099426214535113, 0.7020912402174126, 0.7007214278221743, 0.6965959069053447, 0.6908229709551202, 0.6935330697807089, 0.6855786616770855, 0.6869376639850177, 0.6789683592522187, 0.6789631833432002, 0.6742795572511883, 0.6774181235590002, 0.6710332065849378, 0.6707767383923009, 0.6654344496338421, 0.6693381763896691], 'acc': [0.329286858974359, 0.49458614050060806, 0.5925168430801382, 0.6564404876675023, 0.7028793711711246, 0.7340591915494401, 0.7563161694127701, 0.7713747193196051, 0.7843479307408435, 0.7929900545778663, 0.8044594161434743, 0.8105951235544464, 0.8196783766250867, 0.8224855630605084, 0.8286413217837665, 0.832451074770486, 0.8350777991658647, 0.8394088867692027, 0.8421358678216233, 0.8468278793711902, 0.8489533205004812, 0.8517605068785356, 0.8542468719540555, 0.8556304138594802, 0.8569738530829659, 0.8607836060696854, 0.8602221687520051, 0.8627285852163007, 0.8631897657622043, 0.8631296118062239, 0.8646535129932628, 0.8699671158356127, 0.8672200833752938, 0.8712504010457506, 0.8704683990825779, 0.873435996131022, 0.873115174809365, 0.8737367661403929, 0.8766241578249584, 0.8737768687649649, 0.8772457491177414, 0.8771655437538629, 0.8812159127748506, 0.8787495989733718, 0.8814966313954459, 0.880634424125762, 0.881917709297655, 0.8821984280138625, 0.8833413538467743, 0.8822385306001895]}\n"
     ]
    }
   ],
   "source": [
    "# Prepare for training \n",
    "model = create_model(act=\"relu\")\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "train = {}\n",
    "\n",
    "# First training for 50 epochs - (0-50)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_1\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=2*epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_1\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.5630 - acc: 0.9179 - val_loss: 0.5595 - val_acc: 0.92082:17 - los - ETA: 1:34 - loss: 0.603 - ETA: 1:06 - loss: 0.5934 - - ETA: 1:02 - loss - ETA: 58s - loss: 0.5873 - acc:  - ETA: 57s - loss: 0.58 - ETA: 11s - loss: - ETA: 8s - loss: 0.5649 - acc - ETA: 6s - lo - ETA: 1s - loss: 0.5630 - acc: 0. - ETA: 0s - loss: 0.5630 - acc: 0.\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.5339 - acc: 0.9257 - val_loss: 0.5404 - val_acc: 0.9253. - ETA: - ETA:\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.5138 - acc: 0.9314 - val_loss: 0.5480 - val_acc: 0.92178s - loss: 0.5136 - acc: - ETA: 7s - loss: 0.51 - ETA: 4s - los\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.5015 - acc: 0.9334 - val_loss: 0.5185 - val_acc: 0.93042s - loss: 0.501\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4928 - acc: 0.9339 - val_loss: 0.5178 - val_acc: 0.9283s -  - ETA: 2s - loss: 0.493\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4830 - acc: 0.9355 - val_loss: 0.5254 - val_acc: 0.9244\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4803 - acc: 0.9364 - val_loss: 0.5095 - val_acc: 0.9288\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4722 - acc: 0.9379 - val_loss: 0.5016 - val_acc: 0.9300\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4594 - acc: 0.9409 - val_loss: 0.4931 - val_acc: 0.93111 -  - ET - ETA - ETA: 41s - loss: 0.4597 - acc: 0.94 - ETA: 41s - loss: 0.45 - ETA: 24s - loss:  - ETA: 22s  - E - ETA: 16s - loss: 0. - ETA: 14s - loss: 0.4592 - - ETA: 13s - loss: 0.45 - ETA: \n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4566 - acc: 0.9408 - val_loss: 0.4875 - val_acc: 0.9331s: 0.4560 - acc: 0.94 - ETA: 1:07 - loss: 0.4570 - acc: - ETA: 1:06 - loss: 0.4565 - acc: 0.94 - ETA: 1:06 - loss: 0 - ETA: - ETA: 23s - loss: 0.4571 - ETA: 10s - lo - ETA: 5s - loss: 0.4562 - acc: 0.9 - ETA: 5s - loss: 0.4562 - acc: 0.941 - ETA: 4s - l\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4482 - acc: 0.9447 - val_loss: 0.4995 - val_acc: 0.9279 ETA: 43s  - ETA: 40s - loss: 0.4482 - a - ETA: 39s - loss: 0.4481 - acc - ETA: 35s - loss: 0.4491 - acc - - ETA: 4s - l\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4447 - acc: 0.9428 - val_loss: 0.4948 - val_acc: 0.9271ETA: 1:14 - loss: 0.4383 - acc: 0.944  - ETA: 35s - loss: 0. - ETA: 29s - loss: 0. - ETA - - ETA: 13s - lo - ET - ETA: 5s - loss: 0 - ETA: 1s - loss: 0.4438 - ac\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4393 - acc: 0.9431 - val_loss: 0.4807 - val_acc: 0.9328 14s - loss:  - ETA: 7s - loss: 0.4395 - acc - ETA: 5s -  - ETA: 0s - loss: 0.4391 - acc: 0.94 - ETA: 0s - loss: 0.4395 - acc: 0.943\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4344 - acc: 0.9445 - val_loss: 0.4868 - val_acc: 0.9303 ETA: 3s - loss: 0.4344 - acc: 0. - ETA: 2s - loss: 0.434\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4318 - acc: 0.9451 - val_loss: 0.4707 - val_acc: 0.9356A: 1:10 - loss: 0.4 - ETA: 1:07 - loss: 0.4269 - acc: 0. - ETA: 1:07 - loss: 0.4229 - acc - ETA: 1:05 - loss: 0.4268  - ETA: 1:03  - ETA: 54s - lo - ETA: 2s - loss: 0.4320 - acc - ETA: 0s - loss: 0.4322 - acc: 0.9\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4238 - acc: 0.9460 - val_loss: 0.4699 - val_acc: 0.9342s: 0.4235 - ETA: 14s - loss: 0.4244 - acc: 0.94 - ETA: 14s - loss:  - ETA: 12s - loss: 0.4241 - acc - ETA: 11s - loss: 0.4248 - acc - ETA: 10s - loss: 0.42 - ETA: 0s - loss: 0.4240 - acc: 0.\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.4187 - acc: 0.9484 - val_loss: 0.4819 - val_acc: 0.9287- loss: 0.4383 - acc:  - ETA: 1: - ETA: 54s -  - ETA: 52s - loss: 0.4156 - acc: 0.95 - ETA: 52s - loss: 0.4148 - - ETA: 51s - loss: 0.4147 - acc - ETA: 50s - loss: 0.4145 - a - ETA: 49 - ETA: 46s - loss: 0.4123 - - ETA: 45s - loss - ETA: 39s - loss: 0.41 - ETA: 33s - loss - ETA: 16s - loss: 0.4170 - acc - ETA: 15s - loss: 0.4169 - ETA: 14s - loss: 0.4171  - ETA: 7s - ETA: 2s - loss: 0.4189 - acc: 0.94 - ETA: 1s - loss: 0.4188 - ac\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4168 - acc: 0.9470 - val_loss: 0.4666 - val_acc: 0.9353: 0.4168 - acc: 0.94\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4117 - acc: 0.9476 - val_loss: 0.4686 - val_acc: 0.9326 11s - loss: 0.4118 - - ETA: 9s - loss: 0.4119 - acc - ETA: 0s - loss: 0.4116 - acc: 0.94 - ETA: 0s - loss: 0.4117 - acc: 0.947\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4051 - acc: 0.9488 - val_loss: 0.4537 - val_acc: 0.9363A: 10s - loss: 0.4051 - acc: 0. - ETA: 10s - loss: 0.4051\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3987 - acc: 0.9507 - val_loss: 0.4461 - val_acc: 0.9369oss: 0.3997 - - ETA: 14s - loss: 0. - ETA: 13s - loss: 0.3992 - ETA: 5s - loss: 0.3992 - acc - ETA: 4s - loss:\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3986 - acc: 0.9503 - val_loss: 0.4469 - val_acc: 0.9379\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3937 - acc: 0.9504 - val_loss: 0.4462 - val_acc: 0.9373: 5s - loss: - ETA: 1s - loss: 0.3936 - acc\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3911 - acc: 0.9502 - val_loss: 0.4467 - val_acc: 0.9367 lo - ETA: 12s -  - ETA: 10s - loss: 0.3907 - acc: - ETA: 9s - loss: 0.3906 - acc:  - ETA: 8s - loss: 0. - ETA: 4s - lo\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3873 - acc: 0.9517 - val_loss: 0.4549 - val_acc: 0.9325A: 47s -  - ETA: 36s -  - ETA: 34s - loss: 0.3866 - acc: 0. - ETA: 34s - lo - ETA:  - ETA - ETA: 21s - loss: 0.38 - ETA: 6s - loss: 0.3869 - acc: 0.95 - ETA: 6s -  - ETA: 0s - loss: 0.3874 - acc: 0\n",
      "{'val_loss': [0.5594911217212677, 0.540420709848404, 0.5480135992050171, 0.5184911138057708, 0.5177814515590667, 0.525421714925766, 0.5094652741909027, 0.501620380115509, 0.49307598848342893, 0.4874684368610382, 0.4995268879890442, 0.49480370492935183, 0.48067618737220763, 0.48681386733055115, 0.4706835277557373, 0.46991068778038025, 0.4818645356655121, 0.46655456433296205, 0.46862304348945616, 0.4537186270236969, 0.4460983902454376, 0.4468611700534821, 0.44618262605667114, 0.4467319087982178, 0.45487737202644346], 'val_acc': [0.9208, 0.9253, 0.9217, 0.9304, 0.9283, 0.9244, 0.9288, 0.93, 0.9311, 0.9331, 0.9279, 0.9271, 0.9328, 0.9303, 0.9356, 0.9342, 0.9287, 0.9353, 0.9326, 0.9363, 0.9369, 0.9379, 0.9373, 0.9367, 0.9325], 'loss': [0.5630146440787193, 0.533945812250048, 0.5137734133768127, 0.5015920430832962, 0.49283583376665396, 0.48303931829139213, 0.48028344080392316, 0.47222276592736523, 0.45929338941981024, 0.45648595737577213, 0.4482610299681332, 0.4445277555347598, 0.4393811016117949, 0.434287510604326, 0.4319000170614844, 0.4236878977588327, 0.4186646797009145, 0.41675711261334997, 0.41166652789244407, 0.4050657087826446, 0.3986831505137221, 0.398679058378539, 0.39366376061526714, 0.39110447703013634, 0.3872894434581631], 'acc': [0.9178685897435898, 0.9256697144881632, 0.9313843439586812, 0.9333293230288068, 0.9338506576836701, 0.9354748155086287, 0.9363570741097209, 0.9379210779595765, 0.9409889316268176, 0.9408485723452037, 0.9446382739431475, 0.9427935514535741, 0.9430542188383733, 0.9444578119987167, 0.9450794033106221, 0.9461020211550836, 0.9484079242670501, 0.947024382438115, 0.9476459736735308, 0.9488089509334632, 0.9506737247161999, 0.950252646737501, 0.9503529034136655, 0.9501924927432758, 0.9516762913248651]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more - (50-75)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_2\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_2\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.3784 - acc: 0.9547 - val_loss: 0.4370 - val_acc: 0.9395 0.\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3737 - acc: 0.9560 - val_loss: 0.4357 - val_acc: 0.9402- ETA: 15s - loss: 0.3749 - ETA: 13s - loss: 0.37 - ETA: 12s -  - ETA: 2s - loss: 0.3739 - acc: - ETA: 0s - loss: 0.3737 - acc: 0.\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3727 - acc: 0.9564 - val_loss: 0.4369 - val_acc: 0.9399ETA: 1:10 - loss: 0.3772 - acc: 0.96 - ETA: 1:11 - los - ETA: 2s - loss: 0.3728 - acc: 0. - ETA: 1s - loss: 0.3729 - acc: 0. - ETA: 0s - loss: 0.3727 - acc: 0. - ETA: 0s - loss: 0.3726 - acc: 0.956\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3698 - acc: 0.9578 - val_loss: 0.4351 - val_acc: 0.93970s - loss: 0.3700 - acc: 0.95\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3693 - acc: 0.9577 - val_loss: 0.4341 - val_acc: 0.9398: 19s - loss: 0.3717 - ETA: 18s - loss - ETA - ETA: 12s - loss: 0.37 - ETA: 11s - loss: 0.3714  - ETA: 2s - loss: 0.3695 - a - ETA: 0s - loss: 0.3694 - acc: 0.95\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3700 - acc: 0.9572 - val_loss: 0.4313 - val_acc: 0.9405 46s - loss: 0.3749 - - ETA: 4s - loss:\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3690 - acc: 0.9572 - val_loss: 0.4342 - val_acc: 0.9408ETA: 1:11 - loss: 0.3 - ETA: 1:09 - loss: 0.3748 - acc: 0.959 - ETA: 1:09 - loss: 0.3749 - acc: 0 - ETA: 1:08 - loss: 0.3770  - ETA: 1:06 - loss: 0.3675 - a - ETA: 1:04 - loss: 0.3670 - acc: 0.95 - ETA: 1:04 - loss: 0.3656 - acc: 0 - ETA: 1:03 - loss: 0.365 - ETA: 52s - lo - ETA: 42 - ETA: 39s - loss - ETA: 33 - ETA - ETA: 19s - loss - ETA: 17s - loss: 0.3675 - acc:  - ETA: 7s - lo - ETA: 2s - loss: 0.368\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3667 - acc: 0.9578 - val_loss: 0.4348 - val_acc: 0.9400s: 0.3 - ETA: 1:05 - loss: 0.3721 - acc: 0.9 - ETA: 1:05 - loss: 0.3704 - acc: 0.9 - ETA: 1:04 - loss: 0.3705 - acc: 0.957 - ETA: 1:04 - loss: 0.3699 - acc: 0.957 - ETA: 1:04 - loss: 0.3695 - acc:  - ETA:  - ETA: 15s - loss: 0.3660 - ETA: 14s - loss: 0. - ETA: 12s  - ETA: 9s - loss: 0.3661 - acc: 0.95 - ETA: 9s - loss: 0.3661 - acc: 0.958 - ETA: 8s - loss: 0.3661 - acc - ETA: 7s - loss: 0.36 - ETA: 4s - loss\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3667 - acc: 0.9581 - val_loss: 0.4350 - val_acc: 0.9387\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3683 - acc: 0.9578 - val_loss: 0.4314 - val_acc: 0.9399 - loss: 0.3680 - acc: 0 - ETA: 0s - loss: 0.3684 - acc: 0.957\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3661 - acc: 0.9578 - val_loss: 0.4364 - val_acc: 0.9392loss: 0.3662 - acc: 0.9 - ETA: 0s - loss: 0.3662 - acc: 0.9\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3640 - acc: 0.9591 - val_loss: 0.4322 - val_acc: 0.9407 loss: 0.3608 - a - ETA: 27s  - ETA: 25s -  - ETA: 18 - ETA: 1\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3668 - acc: 0.9577 - val_loss: 0.4337 - val_acc: 0.9396s - loss: 0.3668 - acc - ETA: 1s - loss: 0.3666 - acc:\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3624 - acc: 0.9593 - val_loss: 0.4312 - val_acc: 0.9403- ETA: 20s - loss: 0.3620 - a - - ETA: 11s - loss - ETA: 9s - loss: 0.3624 - acc: 0.9 - ETA: - ETA: 2s - loss: 0.3623\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3676 - acc: 0.9569 - val_loss: 0.4330 - val_acc: 0.9393 - loss: 0.3685 - acc - ETA: 40s - loss: 0.3681 - acc:  - ETA: 40s - lo - ETA: 38s - loss: 0.3694 - a - ETA: 37s - loss - ETA: 34s - loss: 0.3696 - acc:  - ETA: 34s - loss: 0.3 - ETA: 1s - loss: 0.3678 - acc: 0 - ETA: 0s - loss: 0.3676 - acc: 0.956\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3633 - acc: 0.9589 - val_loss: 0.4315 - val_acc: 0.9396A:  - E - ETA: 9s - loss: 0.3637 - acc:  - ETA: 8s - loss: 0.363 - ETA: 5s - loss: 0.3628 -  - ETA: 3s - loss: 0.\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3596 - acc: 0.9598 - val_loss: 0.4320 - val_acc: 0.9388ss: 0.3609 -  - ETA: 48s - loss: 0.3608 - acc:  - ETA: 48s  - ETA: 18s - loss: 0.3599 - acc - ETA: 17s  - ETA: 11s - loss: 0.359 -  - ETA: 2s - loss: 0.3595 - acc: 0.959 - ETA: 2s - loss: 0.3594 - \n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3632 - acc: 0.9586 - val_loss: 0.4333 - val_acc: 0.9389\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3613 - acc: 0.9598 - val_loss: 0.4295 - val_acc: 0.9397  - ETA: 12s -  - ETA: 9s - loss: 0.3610 - acc: 0 - ETA: 8s - loss: 0.3 - ETA: 5s - loss: 0.3615 - acc:  - ETA: 4s - loss: 0.3617 - acc: 0.959 - ETA: 3s - loss: 0.3616  - ETA: 1s - loss: 0.3612 - acc:\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3587 - acc: 0.9598 - val_loss: 0.4312 - val_acc: 0.9400oss: 0.3586 - acc: 0.95\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3608 - acc: 0.9588 - val_loss: 0.4288 - val_acc: 0.9412\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3604 - acc: 0.9584 - val_loss: 0.4297 - val_acc: 0.9409c: 0.958 - ETA: 2s - loss: 0.3604 -\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3591 - acc: 0.9595 - val_loss: 0.4302 - val_acc: 0.94000. - ETA: 43s - loss: 0. - ETA: 42s - loss: 0.3605 - ETA: 40s - loss: 0.3598 - a - - ETA: 24s  - ETA: 13s - lo - ETA: 11s - loss: 0.3585 - acc: 0. - ETA: 11s - loss: 0.3585 - - ETA: 10s - loss: 0.3583 - acc: 0.9 - ETA: 9s - loss: 0.3583 - acc: 0. - ETA: 8s - loss:  - ETA: 5s - loss: 0.3583 - acc: 0. - ETA: 4s - loss: 0.3585 - - ETA: 1s - loss: 0.3587 - a\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3576 - acc: 0.9600 - val_loss: 0.4309 - val_acc: 0.9406. - ETA - ETA: 38s - loss: 0. - ETA: 24s - loss: 0.3577 - ETA:  - ETA:  - ETA - ETA: 9s - loss: 0.3581 - acc: 0 - ETA: 8s - loss: 0.3581 - acc: - - ETA: 0s - loss: 0.3576 - acc: 0.959\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3568 - acc: 0.9604 - val_loss: 0.4291 - val_acc: 0.9399\n",
      "{'val_loss': [0.43698849267959594, 0.43566109290122984, 0.43686207990646364, 0.4351421065330505, 0.4340761528015137, 0.4312846390247345, 0.4342050701141357, 0.4348142623901367, 0.43497609553337097, 0.4313897805213928, 0.4364479069709778, 0.43224830780029294, 0.433659273815155, 0.43121507840156553, 0.432983224773407, 0.43149812994003295, 0.43196640996932983, 0.43325454139709474, 0.4295385568618774, 0.43124645648002624, 0.42875798845291135, 0.42966645202636716, 0.43017170667648313, 0.4309269227027893, 0.4290649941444397], 'val_acc': [0.9395, 0.9402, 0.9399, 0.9397, 0.9398, 0.9405, 0.9408, 0.94, 0.9387, 0.9399, 0.9392, 0.9407, 0.9396, 0.9403, 0.9393, 0.9396, 0.9388, 0.9389, 0.9397, 0.94, 0.9412, 0.9409, 0.94, 0.9406, 0.9399], 'loss': [0.37841355089957895, 0.3736902769842628, 0.3726609191490665, 0.3697359597870353, 0.3693891435450455, 0.3700256641568149, 0.3688949816189479, 0.36668537385501193, 0.3667109216066519, 0.3683161935746612, 0.36604206624955066, 0.36401980971922887, 0.36675152628131885, 0.36228183411924664, 0.367634231439186, 0.3632343911210581, 0.3596778136818769, 0.3631735254278694, 0.3613147844115549, 0.3587239535814071, 0.3608053484080042, 0.3603481928392752, 0.359009022346704, 0.3575504442648816, 0.3567958417556819], 'acc': [0.9547275641025641, 0.955967276208019, 0.9564084055181264, 0.9578521013604092, 0.957671639396856, 0.9571904074048093, 0.957230510105871, 0.9577919473470615, 0.9580526146553707, 0.9578521013795316, 0.9578521013604092, 0.9590752326336892, 0.9577117420405503, 0.95931584861059, 0.9568695861022749, 0.9589348732755855, 0.9597770291947385, 0.9585538979405809, 0.9597970805643918, 0.9598171318958002, 0.9588346165802986, 0.9583934873084361, 0.9595364132178377, 0.9599775425088226, 0.9604186717806851]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more - (75-100)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_3\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_3\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 214ms/step - loss: 0.3579 - acc: 0.9593 - val_loss: 0.4289 - val_acc: 0.9407s: 0.35\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3547 - acc: 0.9611 - val_loss: 0.4295 - val_acc: 0.9403\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 204ms/step - loss: 0.3573 - acc: 0.9614 - val_loss: 0.4288 - val_acc: 0.9401\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3567 - acc: 0.9605 - val_loss: 0.4289 - val_acc: 0.9404 5s - loss: 0.3569 -  - ETA: 3s - loss: 0.35\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3559 - acc: 0.9590 - val_loss: 0.4287 - val_acc: 0.94056 - acc: 0. - ETA: 1s - loss: 0.3556 - acc:\n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3558 - acc: 0.9604 - val_loss: 0.4289 - val_acc: 0.9404: 1:03 - loss: 0.359 - ETA: 1:00  - ETA: 49s - loss: 0.35 - ETA: 48s - loss: 0.35 - ETA:  - ETA: 39s - loss:  - ETA:  - ETA: 30s - loss: 0.3540 - a - ET - ETA: 3s - loss: 0.3548 - acc: 0. - ETA: 2s - loss: 0.3553\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3581 - acc: 0.9597 - val_loss: 0.4283 - val_acc: 0.9409\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3544 - acc: 0.9616 - val_loss: 0.4283 - val_acc: 0.9406\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3598 - acc: 0.9591 - val_loss: 0.4290 - val_acc: 0.9405\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3576 - acc: 0.9595 - val_loss: 0.4287 - val_acc: 0.9400s - loss: 0.3560 - acc:  - ETA: 0s - loss: 0.3578 - acc: 0.\n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3572 - acc: 0.9612 - val_loss: 0.4287 - val_acc: 0.940209 - - ETA: 1:06 - loss: 0.3628 - acc - ETA: 11s - loss: 0 - ETA: 9s - loss: 0.3574 - - ETA: 7s - loss: 0.3571 -  - ETA: 4s - l\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3555 - acc: 0.9607 - val_loss: 0.4294 - val_acc: 0.94003555 - - ETA: 30s - lo - ETA: 23s - lo - ETA: 17s - loss: 0.3544 - acc: 0. - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.3544 - acc:  - ETA: 15s - loss: 0. - ETA: 13s - loss: 0.3546 - acc:  - ETA:  - ETA: 1s - loss: 0.3550 - acc:\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3593 - acc: 0.9597 - val_loss: 0.4283 - val_acc: 0.9404.3593 - acc: 0. - ETA: 1s - loss: 0.3594 - ac\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3544 - acc: 0.9609 - val_loss: 0.4281 - val_acc: 0.9404TA: 1:11 - loss: 0.3569 - acc: 0.96 - ETA: 1:11 - loss: 0.3630 - ETA: 1:09 -  - ETA: 4s - los\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3562 - acc: 0.9610 - val_loss: 0.4282 - val_acc: 0.9408\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3560 - acc: 0.9605 - val_loss: 0.4290 - val_acc: 0.9402s - loss: 0 - ETA: 6s - loss: 0.3 - ETA: 3s - loss: 0.3\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3560 - acc: 0.9610 - val_loss: 0.4286 - val_acc: 0.9408.961 - ETA: 8s - loss: 0.3561 - acc: 0 - ETA: 7s - loss: 0.3560 - ETA: 4s - loss: 0.3560 - acc: 0.9 - ETA: 4s - loss:\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3561 - acc: 0.9609 - val_loss: 0.4293 - val_acc: 0.9399 0.3559 \n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3554 - acc: 0.9608 - val_loss: 0.4285 - val_acc: 0.9408s -\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3564 - acc: 0.9607 - val_loss: 0.4286 - val_acc: 0.9406 - ETA: 1s - loss: 0.3563 - acc:\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3544 - acc: 0.9606 - val_loss: 0.4288 - val_acc: 0.9406\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3560 - acc: 0.9607 - val_loss: 0.4280 - val_acc: 0.9404oss: 0.3563 - acc: 0.9\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3582 - acc: 0.9603 - val_loss: 0.4284 - val_acc: 0.9408\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3542 - acc: 0.9615 - val_loss: 0.4278 - val_acc: 0.9412\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.3541 - acc: 0.9617 - val_loss: 0.4277 - val_acc: 0.9411A: 27s - loss: 0.3543 - acc:  - - ETA: 19s  - ETA: 12s - loss:  - ETA: 10s - loss: 0.3533 - acc: 0. - ETA: 10s - loss: 0.3 - ETA: 7s - loss: 0.3534 - acc:  - ET\n",
      "{'val_loss': [0.42890940618515017, 0.429476281785965, 0.4287737816810608, 0.42891113886833193, 0.4287343542575836, 0.4288902395248413, 0.42828984575271606, 0.4283210072517395, 0.42896775002479554, 0.4287059875488281, 0.42868418216705323, 0.42943099389076234, 0.42832710900306703, 0.42805296721458436, 0.42820959496498107, 0.42900065307617186, 0.4285962327480316, 0.42933241634368896, 0.428498144197464, 0.4285720456123352, 0.42881690883636475, 0.42801044244766234, 0.4283667218208313, 0.4278276875972748, 0.42774507489204405], 'val_acc': [0.9407, 0.9403, 0.9401, 0.9404, 0.9405, 0.9404, 0.9409, 0.9406, 0.9405, 0.94, 0.9402, 0.94, 0.9404, 0.9404, 0.9408, 0.9402, 0.9408, 0.9399, 0.9408, 0.9406, 0.9406, 0.9404, 0.9408, 0.9412, 0.9411], 'loss': [0.35788610722774117, 0.35471735087235, 0.35739470235728515, 0.35658860976282203, 0.3559105258397961, 0.3557594267554034, 0.35813543530354947, 0.35444176422696794, 0.3596729396205703, 0.3576442110048948, 0.35712298271860266, 0.35555150925176127, 0.35929490080390714, 0.3544595976708828, 0.3561909338354797, 0.3559790760288263, 0.3559313318382902, 0.35606786016856473, 0.3554000453207789, 0.3564172502622644, 0.35445933168730676, 0.356003798083078, 0.35817189386038584, 0.3542086050837018, 0.35403610186046003], 'acc': [0.9592948717948718, 0.9611405197113877, 0.9613811357265334, 0.9604988771254411, 0.9590150786012192, 0.9603986204492767, 0.9596767725759414, 0.9616217517225567, 0.9591754892907313, 0.9595163618673068, 0.9612006737438578, 0.9606392364452999, 0.9596767725376965, 0.9609199551232626, 0.9610202117038148, 0.9605590311005439, 0.9610402630352232, 0.9608999037536092, 0.9608196984662205, 0.9606592878149532, 0.9606392364452999, 0.9606392364835449, 0.9602783124608262, 0.9614813924026979, 0.9617220083413539]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more  - (100-125)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1*0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_4\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_trained_model.h5\")\n",
    "print(train[\"part_4\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Final Logs:  {'part_1': <keras.callbacks.History object at 0x000002DBAE21FE80>, 'part_2': <keras.callbacks.History object at 0x000002DBB1226940>, 'part_3': <keras.callbacks.History object at 0x000002DBAE242D68>, 'part_4': <keras.callbacks.History object at 0x000002DBF553F240>}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n \\n Final Logs: \", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
