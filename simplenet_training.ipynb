{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Replicating the SimpleNet model architecture. \"\"\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import glorot_normal, RandomNormal, Zeros\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Retrieval & mean/std preprocess\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model architecture\n",
    "def create_model(s = 2, weight_decay = 1e-2, act=\"relu\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal(), input_shape=x_train.shape[1:]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 4\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    # First Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 5\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 6\n",
    "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 7\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    # Second Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Block 8\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 9\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    # Third Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    \n",
    "    \n",
    "    # Block 10\n",
    "    model.add(Conv2D(512, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 11  \n",
    "    model.add(Conv2D(2048, (1,1), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Block 12  \n",
    "    model.add(Conv2D(256, (1,1), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fourth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    # Block 13\n",
    "    model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.005), kernel_initializer=glorot_normal()))\n",
    "    model.add(Activation(act))\n",
    "    # Fifth Maxpooling\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=s))\n",
    "\n",
    "    # Final Classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "390/390 [==============================] - 87s 223ms/step - loss: 13.3755 - acc: 0.3261 - val_loss: 12.1981 - val_acc: 0.165813 - ac - ETA: 18s - loss: 13.9671 - acc  - ETA: 8s - loss: 13.66 - ETA: 7s - l - ETA: 0s - loss: 13.3826 - acc: 0.32\n",
      "Epoch 2/50\n",
      "390/390 [==============================] - 81s 207ms/step - loss: 8.7435 - acc: 0.4899 - val_loss: 8.1817 - val_acc: 0.2400\n",
      "Epoch 3/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 5.7768 - acc: 0.5936 - val_loss: 5.5061 - val_acc: 0.40195.8794 - acc: 0. - ETA: 6s - loss: 5.8682 - acc: 0. - ETA: 6s  - ETA: 0s - loss: 5.7852 - acc: 0.5\n",
      "Epoch 4/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 4.0173 - acc: 0.6623 - val_loss: 3.6880 - val_acc: 0.5883\n",
      "Epoch 5/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 2.9550 - acc: 0.7098 - val_loss: 2.6577 - val_acc: 0.7044\n",
      "Epoch 6/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 2.3053 - acc: 0.7417 - val_loss: 2.3567 - val_acc: 0.6674\n",
      "Epoch 7/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.8808 - acc: 0.7673 - val_loss: 1.6961 - val_acc: 0.7895\n",
      "Epoch 8/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.6034 - acc: 0.7834 - val_loss: 1.8126 - val_acc: 0.6862\n",
      "Epoch 9/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.4029 - acc: 0.7982 - val_loss: 1.4172 - val_acc: 0.7698\n",
      "Epoch 10/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.2634 - acc: 0.8069 - val_loss: 1.2486 - val_acc: 0.7960\n",
      "Epoch 11/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.1567 - acc: 0.8151 - val_loss: 1.6621 - val_acc: 0.6822- loss: 1.1589 - acc: 0.814 - ETA: 3s - loss: 1.15\n",
      "Epoch 12/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.0766 - acc: 0.8253 - val_loss: 1.2759 - val_acc: 0.7599ET - ET\n",
      "Epoch 13/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 1.0104 - acc: 0.8320 - val_loss: 1.0506 - val_acc: 0.8122- loss: 1.0112 - acc: 0. - ETA: 4s - loss: 1.0114 - acc: 0 - ETA: 3s - loss: \n",
      "Epoch 14/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.9599 - acc: 0.8368 - val_loss: 0.9981 - val_acc: 0.8214\n",
      "Epoch 15/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.9227 - acc: 0.8405 - val_loss: 1.0868 - val_acc: 0.7807\n",
      "Epoch 16/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8810 - acc: 0.8491 - val_loss: 1.0016 - val_acc: 0.8092\n",
      "Epoch 17/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8638 - acc: 0.8474 - val_loss: 0.9713 - val_acc: 0.8115ss: 0.8627 - a\n",
      "Epoch 18/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8332 - acc: 0.8527 - val_loss: 0.9323 - val_acc: 0.8192\n",
      "Epoch 19/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.8100 - acc: 0.8550 - val_loss: 0.8319 - val_acc: 0.8535- ETA: 14s - loss: 0.8123 - acc: 0.85 - ET - ETA: 3s - loss: 0.80\n",
      "Epoch 20/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7892 - acc: 0.8593 - val_loss: 0.8793 - val_acc: 0.8294\n",
      "Epoch 21/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7752 - acc: 0.8638 - val_loss: 0.7942 - val_acc: 0.8642\n",
      "Epoch 22/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7597 - acc: 0.8651 - val_loss: 0.9757 - val_acc: 0.7949\n",
      "Epoch 23/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7484 - acc: 0.8686 - val_loss: 0.7959 - val_acc: 0.8490 - acc - ETA: 5s - loss: 0.7492 - ac - ETA: 3s - loss: \n",
      "Epoch 24/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7355 - acc: 0.8692 - val_loss: 0.7099 - val_acc: 0.8842\n",
      "Epoch 25/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7299 - acc: 0.8690 - val_loss: 0.7262 - val_acc: 0.8729\n",
      "Epoch 26/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7179 - acc: 0.8737 - val_loss: 0.7610 - val_acc: 0.8603\n",
      "Epoch 27/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7080 - acc: 0.8750 - val_loss: 0.8030 - val_acc: 0.8480\n",
      "Epoch 28/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.7000 - acc: 0.8771 - val_loss: 0.7374 - val_acc: 0.86300.6997 - acc: 0.\n",
      "Epoch 29/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6959 - acc: 0.8777 - val_loss: 0.8546 - val_acc: 0.8333\n",
      "Epoch 30/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6881 - acc: 0.8801 - val_loss: 0.7830 - val_acc: 0.8473s: 0.6844 - acc: 0.88 - ETA: \n",
      "Epoch 31/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6853 - acc: 0.8804 - val_loss: 0.7440 - val_acc: 0.8606\n",
      "Epoch 32/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6782 - acc: 0.8820 - val_loss: 0.6989 - val_acc: 0.8752\n",
      "Epoch 33/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6746 - acc: 0.8825 - val_loss: 0.7139 - val_acc: 0.8691- loss: 0.6739 - ac - ETA\n",
      "Epoch 34/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6795 - acc: 0.8798 - val_loss: 0.7668 - val_acc: 0.8528 ETA: 4s - l\n",
      "Epoch 35/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6662 - acc: 0.8858 - val_loss: 0.7720 - val_acc: 0.8509 0.\n",
      "Epoch 36/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6676 - acc: 0.8859 - val_loss: 0.8221 - val_acc: 0.83843s - loss: 0.6691 -  - ETA: 1s - loss: 0.6680 - acc: \n",
      "Epoch 37/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6583 - acc: 0.8877 - val_loss: 0.7688 - val_acc: 0.8551s: 0.6600 - acc: 0 - ETA: 3s - loss: 0.6600 - acc:  - ETA: 1s - loss: 0.6597 - a\n",
      "Epoch 38/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6533 - acc: 0.8884 - val_loss: 0.8719 - val_acc: 0.8215\n",
      "Epoch 39/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6556 - acc: 0.8875 - val_loss: 0.7698 - val_acc: 0.8564A: 1:03 - loss: 0.66 - ETA: 8s - loss: 0.6553  - ETA: 5\n",
      "Epoch 40/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6484 - acc: 0.8903 - val_loss: 0.8961 - val_acc: 0.8159TA: 0s - loss: 0.6485 - acc: 0.890\n",
      "Epoch 41/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6393 - acc: 0.8928 - val_loss: 0.7965 - val_acc: 0.8406\n",
      "Epoch 42/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6473 - acc: 0.8903 - val_loss: 0.7423 - val_acc: 0.8631\n",
      "Epoch 43/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6388 - acc: 0.8930 - val_loss: 0.6714 - val_acc: 0.8820\n",
      "Epoch 44/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6393 - acc: 0.8923 - val_loss: 0.7598 - val_acc: 0.8555\n",
      "Epoch 45/50\n",
      "390/390 [==============================] - 80s 206ms/step - loss: 0.6384 - acc: 0.8920 - val_loss: 0.7792 - val_acc: 0.8422\n",
      "Epoch 46/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6377 - acc: 0.8932 - val_loss: 0.7692 - val_acc: 0.8466\n",
      "Epoch 47/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6353 - acc: 0.8941 - val_loss: 0.7188 - val_acc: 0.8628\n",
      "Epoch 48/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6296 - acc: 0.8965 - val_loss: 0.7355 - val_acc: 0.8604\n",
      "Epoch 49/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6310 - acc: 0.8947 - val_loss: 0.7162 - val_acc: 0.8685acc - ETA: 17s - loss: 0.6303  - ETA: 7s - loss: 0.6308 - acc: 0 - ETA: 6s - loss: 0.6310 - acc: 0.894 - ETA: 6s - loss: 0 - ETA: 2s - loss: 0.6309 - acc: 0.89 - ETA: 2s - loss: 0.6310 - a - ETA: 0s - loss: 0.6308 - acc: 0.894\n",
      "Epoch 50/50\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.6299 - acc: 0.8954 - val_loss: 0.6709 - val_acc: 0.8856\n",
      "{'val_loss': [12.198139149475098, 8.181736422729491, 5.5061020713806155, 3.687987009048462, 2.6577195014953614, 2.356749296569824, 1.6960634174346925, 1.8126187757492065, 1.4172241865158082, 1.2486346187591553, 1.6620957601547242, 1.2758680280685424, 1.0505947236061097, 0.998128925037384, 1.0867524242401123, 1.0015752697944642, 0.9712963147163391, 0.932343389415741, 0.8318700055122376, 0.8793217713356019, 0.7941833787918091, 0.9757387542724609, 0.7959336821556091, 0.7099367329597474, 0.7261663988113404, 0.7610173940658569, 0.8030403770446778, 0.7374253973007202, 0.8545896901130676, 0.7829661057472229, 0.743978221988678, 0.6989083923339844, 0.7139068077087403, 0.766843782711029, 0.7720231631278992, 0.8220767733573914, 0.7687939198493957, 0.8719407111167907, 0.7698108596801758, 0.896140110206604, 0.7964546062469482, 0.7422838307380676, 0.6713697590827942, 0.7598311212539672, 0.7792295936584472, 0.7692011964797973, 0.7187779209136963, 0.735491967010498, 0.7162310241699219, 0.6709230602264404], 'val_acc': [0.1658, 0.24, 0.4019, 0.5883, 0.7044, 0.6674, 0.7895, 0.6862, 0.7698, 0.796, 0.6822, 0.7599, 0.8122, 0.8214, 0.7807, 0.8092, 0.8115, 0.8192, 0.8535, 0.8294, 0.8642, 0.7949, 0.849, 0.8842, 0.8729, 0.8603, 0.848, 0.863, 0.8333, 0.8473, 0.8606, 0.8752, 0.8691, 0.8528, 0.8509, 0.8384, 0.8551, 0.8215, 0.8564, 0.8159, 0.8406, 0.8631, 0.882, 0.8555, 0.8422, 0.8466, 0.8628, 0.8604, 0.8685, 0.8856], 'loss': [13.375506772750462, 8.741719390935227, 5.775651496688946, 4.016612228996906, 2.9546217836551953, 2.3047841450400868, 1.8806236096976618, 1.6032296194263784, 1.4028047079607047, 1.2633064713736624, 1.156655033314729, 1.0766693765550306, 1.0103919468975464, 0.9598111904007829, 0.9225310611158829, 0.88106727728218, 0.8636953273108651, 0.8331488833849653, 0.810046362578582, 0.7891521759625234, 0.7752831044547406, 0.7595621855701817, 0.7483780662486443, 0.7355863785644277, 0.729975988496218, 0.7178979946475478, 0.7080399225566645, 0.7001055308668744, 0.6959679207053749, 0.6880921639805617, 0.6854207000175576, 0.678087390019873, 0.6746490903110278, 0.679614036214019, 0.6661402075803926, 0.6675548854262775, 0.658449514796805, 0.6534265777718229, 0.655608724909101, 0.6484873226371988, 0.6393121222597611, 0.6473563377735589, 0.6389645143774484, 0.639206465387482, 0.6382845396180163, 0.6377303859611563, 0.6351282334228261, 0.6296113047953175, 0.6309194883016733, 0.6298922618918592], 'acc': [0.32610176282051284, 0.4899542829452664, 0.5936597690469071, 0.662355630394737, 0.709877285870903, 0.7418391080976565, 0.7672040423292895, 0.7834456208274653, 0.7983036573246041, 0.8069257298875857, 0.8151066731022152, 0.8252927494194403, 0.8319497914279085, 0.836802213647865, 0.8405117099584201, 0.8490535771384008, 0.8474895733459125, 0.8528031761691399, 0.8550489252868814, 0.8592797561949325, 0.8638113570741097, 0.8651347449279421, 0.8685635226370243, 0.8691851138915624, 0.8690046519088868, 0.8737768687458424, 0.875, 0.8771053898361275, 0.8776868784087264, 0.8800729868272041, 0.8803938081297387, 0.8819979146806559, 0.8824390439907633, 0.8797120628809753, 0.8858076676291305, 0.885927975617581, 0.887652390080459, 0.8883341353865897, 0.8874518768237423, 0.8902991658837358, 0.8928055823097866, 0.8903392684891853, 0.8930060956238706, 0.8923243502986175, 0.8920636830476756, 0.8931665062942603, 0.8941891241387216, 0.8964950272698107, 0.8947104587362178, 0.895392204023226]}\n"
     ]
    }
   ],
   "source": [
    "# Prepare for training \n",
    "model = create_model(act=\"relu\")\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "train = {}\n",
    "\n",
    "# First training for 50 epochs - (0-50)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_1\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=2*epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_generic_first.h5\")\n",
    "print(train[\"part_1\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "390/390 [==============================] - 83s 213ms/step - loss: 0.5279 - acc: 0.9294 - val_loss: 0.5352 - val_acc: 0.9265\n",
      "Epoch 2/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4940 - acc: 0.9394 - val_loss: 0.5299 - val_acc: 0.9269\n",
      "Epoch 3/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4794 - acc: 0.9415 - val_loss: 0.5272 - val_acc: 0.9278\n",
      "Epoch 4/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4667 - acc: 0.9459 - val_loss: 0.5060 - val_acc: 0.9317\n",
      "Epoch 5/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4577 - acc: 0.9473 - val_loss: 0.5014 - val_acc: 0.9316.4567 - acc:  - ETA: 4s - loss: 0.4566 - ac - ETA: 2s - loss: 0.4570 \n",
      "Epoch 6/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4502 - acc: 0.9488 - val_loss: 0.4975 - val_acc: 0.9321\n",
      "Epoch 7/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4397 - acc: 0.9497 - val_loss: 0.4869 - val_acc: 0.9342\n",
      "Epoch 8/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4331 - acc: 0.9512 - val_loss: 0.4904 - val_acc: 0.9320loss: 0.4331 - acc - ETA: 0s - loss: 0.4331 - acc: 0.95\n",
      "Epoch 9/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4257 - acc: 0.9531 - val_loss: 0.4900 - val_acc: 0.9320\n",
      "Epoch 10/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4185 - acc: 0.9542 - val_loss: 0.4764 - val_acc: 0.9376s: 0.4187 - \n",
      "Epoch 11/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4124 - acc: 0.9550 - val_loss: 0.4681 - val_acc: 0.9384\n",
      "Epoch 12/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4102 - acc: 0.9550 - val_loss: 0.4720 - val_acc: 0.9354\n",
      "Epoch 13/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.4039 - acc: 0.9550 - val_loss: 0.4642 - val_acc: 0.9359\n",
      "Epoch 14/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3971 - acc: 0.9571 - val_loss: 0.4675 - val_acc: 0.9358\n",
      "Epoch 15/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3937 - acc: 0.9579 - val_loss: 0.4660 - val_acc: 0.9350\n",
      "Epoch 16/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3898 - acc: 0.9575 - val_loss: 0.4643 - val_acc: 0.9321ss: 0.3900 -  - ETA: 4s - loss: 0.3897 - acc: 0.95 - ETA: 4s - los\n",
      "Epoch 17/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3849 - acc: 0.9583 - val_loss: 0.4708 - val_acc: 0.93196s - loss: 0.3846 - a - ETA: 4s - loss: 0.3845  - ETA: 1s - loss: 0.3847 - acc\n",
      "Epoch 18/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3813 - acc: 0.9593 - val_loss: 0.4528 - val_acc: 0.9373816 - acc - ETA: 16 - ETA: 0s - loss: 0.3818 - acc: 0\n",
      "Epoch 19/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3753 - acc: 0.9606 - val_loss: 0.4505 - val_acc: 0.9356\n",
      "Epoch 20/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3719 - acc: 0.9602 - val_loss: 0.4457 - val_acc: 0.93640. - ETA: 0s - loss: 0.3721 - acc: 0.96\n",
      "Epoch 21/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3685 - acc: 0.9607 - val_loss: 0.4430 - val_acc: 0.9384\n",
      "Epoch 22/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3626 - acc: 0.9617 - val_loss: 0.4473 - val_acc: 0.9377\n",
      "Epoch 23/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3581 - acc: 0.9630 - val_loss: 0.4432 - val_acc: 0.9384\n",
      "Epoch 24/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3548 - acc: 0.9634 - val_loss: 0.4418 - val_acc: 0.9354\n",
      "Epoch 25/25\n",
      "390/390 [==============================] - 80s 205ms/step - loss: 0.3537 - acc: 0.9631 - val_loss: 0.4350 - val_acc: 0.9379\n",
      "{'val_loss': [0.5351767881393432, 0.5299312427043915, 0.5271883228778839, 0.5060316390037537, 0.5013981282711029, 0.4974909520149231, 0.48690204243659974, 0.49037749404907227, 0.49000645847320556, 0.47639935812950135, 0.46811473579406737, 0.47196283483505247, 0.46415326957702635, 0.4675053162574768, 0.46595163836479186, 0.46431143164634703, 0.47084133262634276, 0.4528141751289368, 0.4505188163280487, 0.44567307901382447, 0.4430270095825195, 0.44731461725234983, 0.44322034678459166, 0.441848805475235, 0.4350217259407043], 'val_acc': [0.9265, 0.9269, 0.9278, 0.9317, 0.9316, 0.9321, 0.9342, 0.932, 0.932, 0.9376, 0.9384, 0.9354, 0.9359, 0.9358, 0.935, 0.9321, 0.9319, 0.9373, 0.9356, 0.9364, 0.9384, 0.9377, 0.9384, 0.9354, 0.9379], 'loss': [0.5278881750045679, 0.49412834045785264, 0.4794874670588133, 0.46674614971671535, 0.4576988572163348, 0.4502433603548797, 0.4397596085362225, 0.4331112908349421, 0.42569929471297413, 0.4184555493469685, 0.4123558517656611, 0.41016055307481625, 0.40383995978121656, 0.3970633538097459, 0.3936651333769736, 0.3897942426707984, 0.3848462195915789, 0.38125039175759917, 0.3753439723240629, 0.3718956624291285, 0.36846015384115394, 0.36259157730479785, 0.358014075083728, 0.35481252409649233, 0.3536023885097724], 'acc': [0.9293870192307693, 0.9393848251715126, 0.941450112249211, 0.9458413538276518, 0.9473051010204654, 0.9487888995446875, 0.9497112608659638, 0.9511950593901859, 0.9531400384985563, 0.9542428617260186, 0.9549446582870679, 0.9550649663520081, 0.9549647096375987, 0.957150304761115, 0.9579323067242876, 0.9575112287647112, 0.9582531279694548, 0.959355951235162, 0.9606191851521364, 0.9601780558611516, 0.9607394931214644, 0.9617420596345174, 0.9630052935514918, 0.963386268829129, 0.963145652890473]}\n"
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more - (50-75)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_2\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_generic_second.h5\")\n",
    "print(train[\"part_2\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "199/390 [==============>...............] - ETA: 40s - loss: 0.3423 - acc: 0.9675 - ETA: 53s - loss: 0.3407 - acc: 0.96 - - ETA: 48s - loss: 0.3399 - acc - ETA: 47s - lo"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4511dbf0e2c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m train[\"part_3\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n\u001b[0;32m      5\u001b[0m                                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                     verbose=1,validation_data=(x_test,y_test))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"simplenet_generic_third.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"part_3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     \u001b[1;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[1;32m--> 709\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[1;34m(uid)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \"\"\"\n\u001b[1;32m--> 626\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1525\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m         \u001b[1;31m# The transformation of images is not under thread lock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1703\u001b[0m         \u001b[1;31m# so it can be done in parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1704\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m   1662\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m         batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n\u001b[1;32m-> 1664\u001b[1;33m                            dtype=self.dtype)\n\u001b[0m\u001b[0;32m   1665\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training for 25 epochs more - (75-100)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_3\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_generic_third.h5\")\n",
    "print(train[\"part_3\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 25 epochs more  - (100-125)\n",
    "opt_adm = keras.optimizers.Adadelta(lr=0.1*0.1*0.1*0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_adm, metrics=['accuracy'])\n",
    "train[\"part_4\"] = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                                    verbose=1,validation_data=(x_test,y_test))\n",
    "model.save(\"simplenet_generic_fourth.h5\")\n",
    "print(train[\"part_4\"].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n \\n Final Logs: \", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
